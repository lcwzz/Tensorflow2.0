{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (60000, 28, 28)\n",
      "x_test.shape (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# 读取数据集\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "train_num = x_train.shape[0]\n",
    "test_num = x_test.shape[0]\n",
    "print(\"x_train.shape:\", x_train.shape)\n",
    "print(\"x_test.shape\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (60000, 28, 28, 1)\n",
      "x_test.shape (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# 维度扩增\n",
    "x_train = tf.expand_dims(x_train, axis=-1)\n",
    "x_test = tf.expand_dims(x_test, axis=-1)\n",
    "print(\"x_train.shape:\", x_train.shape)\n",
    "print(\"x_test.shape\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据类型转换\n",
    "x_train = tf.cast(x_train / 255, tf.float32)\n",
    "y_train = tf.cast(y_train, tf.int64)\n",
    "\n",
    "x_test = tf.cast(x_test / 255, tf.float32)\n",
    "y_test = tf.cast(y_test, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset: <TensorSliceDataset shapes: ((28, 28, 1), ()), types: (tf.float32, tf.int64)>\n",
      "test_dataset: <TensorSliceDataset shapes: ((28, 28, 1), ()), types: (tf.float32, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "# 创建数据集\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "print(\"train_dataset:\", train_dataset)\n",
    "print(\"test_dataset:\", test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset: <BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>\n",
      "test_dataset: <BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "# 用于自定义训练\n",
    "train_dataset_customer = train_dataset.shuffle(train_num).batch(BATCH_SIZE)\n",
    "test_dataset_customer = test_dataset.batch(BATCH_SIZE)\n",
    "# 用于keras训练\n",
    "train_dataset = train_dataset.shuffle(train_num).repeat().batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "print(\"train_dataset:\", train_dataset)\n",
    "print(\"test_dataset:\", test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 32)        4640      \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 5,130\n",
      "Trainable params: 5,130\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 构建模型\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.GlobalMaxPooling2D(),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在keras中使用tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20191214-19_40_51'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.strftime(\"%Y%m%d-%H_%M_%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取当前时间\n",
    "time_str = time.strftime(\"%Y%m%d-%H_%M_%S\", time.localtime())\n",
    "# 建立log目录\n",
    "log_dir = os.path.join('E:\\\\log', time_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立tensorboard回调函数\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard可视化自定义标量\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "# 控制lr变化函数\n",
    "def lr_schedule(epoch):\n",
    "    if epoch < 5:\n",
    "        learning_rate = 0.005\n",
    "    else:\n",
    "        learning_rate = 0.001\n",
    "    # 写入标量变化\n",
    "    tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\n",
    "    return learning_rate\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 937 steps, validate for 156 steps\n",
      "Epoch 1/10\n",
      "  1/937 [..............................] - ETA: 25:45 - loss: 2.3062 - accuracy: 0.1094WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.128474). Check your callbacks.\n",
      "937/937 [==============================] - 8s 8ms/step - loss: 0.5515 - accuracy: 0.8247 - val_loss: 0.2533 - val_accuracy: 0.9239\n",
      "Epoch 2/10\n",
      "937/937 [==============================] - 6s 6ms/step - loss: 0.2585 - accuracy: 0.9179 - val_loss: 0.2012 - val_accuracy: 0.9346\n",
      "Epoch 3/10\n",
      "937/937 [==============================] - 6s 7ms/step - loss: 0.2256 - accuracy: 0.9284 - val_loss: 0.1823 - val_accuracy: 0.9455\n",
      "Epoch 4/10\n",
      "937/937 [==============================] - 6s 6ms/step - loss: 0.2059 - accuracy: 0.9358 - val_loss: 0.1752 - val_accuracy: 0.9453\n",
      "Epoch 5/10\n",
      "937/937 [==============================] - 6s 6ms/step - loss: 0.1888 - accuracy: 0.9394 - val_loss: 0.1822 - val_accuracy: 0.9463\n",
      "Epoch 6/10\n",
      "937/937 [==============================] - 6s 6ms/step - loss: 0.1514 - accuracy: 0.9522 - val_loss: 0.1507 - val_accuracy: 0.9527\n",
      "Epoch 7/10\n",
      "937/937 [==============================] - 6s 6ms/step - loss: 0.1464 - accuracy: 0.9542 - val_loss: 0.1461 - val_accuracy: 0.9556\n",
      "Epoch 8/10\n",
      "937/937 [==============================] - 6s 6ms/step - loss: 0.1445 - accuracy: 0.9548 - val_loss: 0.1503 - val_accuracy: 0.9538\n",
      "Epoch 9/10\n",
      "937/937 [==============================] - 6s 6ms/step - loss: 0.1412 - accuracy: 0.9555 - val_loss: 0.1463 - val_accuracy: 0.9545\n",
      "Epoch 10/10\n",
      "937/937 [==============================] - 6s 6ms/step - loss: 0.1397 - accuracy: 0.9557 - val_loss: 0.1481 - val_accuracy: 0.9549\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2738841f708>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset,\n",
    "          epochs=10, \n",
    "          steps_per_epoch = train_num // BATCH_SIZE, \n",
    "          validation_data = test_dataset, \n",
    "          validation_steps = test_num // BATCH_SIZE,\n",
    "          callbacks=[tensorboard_callback, lr_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 在notebook中显示tensorboard\n",
    "%load_ext tensorboard  \n",
    "%matplotlib inline\n",
    "#### 启动tensorboard\n",
    "%tensorboard --logdir E:\\\\log\n",
    "#### 浏览器中启动tensorboard，在命令行输入  \n",
    "tensorboard --logdir E:\\\\log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义训练中使用Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 32)        4640      \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_1 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 5,130\n",
      "Trainable params: 5,130\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 构建模型\n",
    "model_customer = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.GlobalMaxPooling2D(),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model_customer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个优化器实例\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建loss函数的计算方式\n",
    "loss_func = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建评估训练的指标\n",
    "train_loss = tf.keras.metrics.Mean(\"train_loss\")\n",
    "train_acc = tf.keras.metrics.SparseCategoricalAccuracy(\"train_acc\")\n",
    "test_loss = tf.keras.metrics.Mean(\"test_loss\")\n",
    "test_acc = tf.keras.metrics.SparseCategoricalAccuracy(\"test_acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练一步\n",
    "def train_one_step(model, x, y_true):\n",
    "    with tf.GradientTape() as t:\n",
    "        # 计算损失\n",
    "        y_pred = model(x)\n",
    "        loss = loss_func(y_true, y_pred)\n",
    "    # 计算梯度\n",
    "    gradients = t.gradient(loss, model.trainable_variables)\n",
    "    # 更新参数\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    # 计算训练集的评估指标\n",
    "    train_loss(loss)\n",
    "    train_acc(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试一步\n",
    "def test_one_step(model, x, y_true):\n",
    "    # 计算loss\n",
    "    y_pred = model(x)\n",
    "    loss = loss_func(y_true, y_pred)\n",
    "    # 计算测试集的评估指标\n",
    "    test_loss(loss)\n",
    "    test_acc(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义log保存路径\n",
    "current_time = time.strftime(\"%Y%m%d-%H_%M_%S\", time.localtime())\n",
    "train_log_dir = 'E:/log/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'E:/log/gradient_tape/' + current_time + '/test'\n",
    "# 定义writer\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, test_dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        # 一次取出一个batch进行训练\n",
    "        for (x, y_true) in train_dataset:\n",
    "            train_one_step(model, x, y_true)\n",
    "        # 写入训练数据\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', train_acc.result(), step=epoch)\n",
    "        # 测试\n",
    "        for (x, y_true) in test_dataset:\n",
    "            test_one_step(model, x, y_true)\n",
    "        # 写入测试数据\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', test_acc.result(), step=epoch)\n",
    "            \n",
    "        print(\"Epoch{} train_loss is {:.2f}, train_acc is {:.2f}, test_loss is {:.2f}, test_acc is {:.2f}\".format(epoch+1, \n",
    "                                                                                                                  train_loss.result(),\n",
    "                                                                                                                  train_acc.result(),\n",
    "                                                                                                                  test_loss.result(),\n",
    "                                                                                                                  test_acc.result()))\n",
    "        train_loss.reset_states()\n",
    "        train_acc.reset_states()\n",
    "        test_loss.reset_states()\n",
    "        test_acc.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1 train_loss is 1.05, train_acc is 0.69, test_loss is 0.47, test_acc is 0.86\n",
      "Epoch2 train_loss is 0.42, train_acc is 0.87, test_loss is 0.34, test_acc is 0.89\n",
      "Epoch3 train_loss is 0.34, train_acc is 0.89, test_loss is 0.29, test_acc is 0.91\n",
      "Epoch4 train_loss is 0.30, train_acc is 0.91, test_loss is 0.27, test_acc is 0.92\n",
      "Epoch5 train_loss is 0.27, train_acc is 0.91, test_loss is 0.24, test_acc is 0.92\n",
      "Epoch6 train_loss is 0.26, train_acc is 0.92, test_loss is 0.23, test_acc is 0.93\n",
      "Epoch7 train_loss is 0.24, train_acc is 0.92, test_loss is 0.21, test_acc is 0.93\n",
      "Epoch8 train_loss is 0.23, train_acc is 0.93, test_loss is 0.21, test_acc is 0.93\n",
      "Epoch9 train_loss is 0.22, train_acc is 0.93, test_loss is 0.20, test_acc is 0.94\n",
      "Epoch10 train_loss is 0.21, train_acc is 0.93, test_loss is 0.20, test_acc is 0.94\n"
     ]
    }
   ],
   "source": [
    "train(model_customer, train_dataset_customer, test_dataset_customer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
